# AI Text Autocomplete

Проект по созданию и сравнению моделей автодополнения текста на основе собственных данных.  
Реализованы две архитектуры: простая **LSTM** и предобученная **DistilGPT-2**.

---

## Цель проекта

Создать систему, способную автоматически предсказывать **следующее слово или фразу** по заданному началу текста.  

---

## Модели

| Модель | Тип | Назначение |
|:--|:--|:--|
| **SimpleLSTMNextToken** | Своя LSTM модель | Базовая модель для задачи автодополнения. |
| **DistilGPT-2** | Transformer | Предобученная модель, используемая для сравнения качества и контекстности. |

---

## Важно!!! Данные
Файл raw_dataset.csv (Sentiment140) не загружен в репозиторий из-за ограничения GitHub ( >100 МБ).
Скачать можно отсюда: <https://code.s3.yandex.net/deep-learning/tweets.txt>
После загрузки поместите файл в папку data/raw_dataset.csv

---

## Структура проекта

```bash
ai-text-autocomplete/
├── data/                            # Датасеты
│   ├── raw_dataset.csv              # "сырой" скачанный датасет
│   └── dataset_processed.csv        # "очищенный" датасет
│   ├── train.csv                    # тренировочная выборка
│   ├── val.csv                      # валидационная выборка
│   └── test.csv                     # тестовая выборка
│
├── src/                             # Весь код проекта
│   ├── data_utils.py                # Обработка датасета
|   ├── next_token_dataset.py        # код с torch Dataset'ом 
│   ├── lstm_model.py                # код lstm модели
|   ├── eval_lstm.py                 # замер метрик lstm модели
|   ├── lstm_train.py                # код обучения модели
|   ├── eval_transformer_pipeline.py # код с запуском и замером качества трансформера
│
├── models/                          # веса обученных моделей
|
├── solution.ipynb                   # ноутбук с решением
└── requirements.txt                 # зависимости проекта
