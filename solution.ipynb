{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6abdba",
   "metadata": {},
   "source": [
    "#### Этап 0. Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1252de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Автоперезагрузка импортов\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Отключаем параллелизм в токенизаторах (иначе могут быть проблемы с многопроцессорностью)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Подключаем нужные пути\n",
    "SRC_DIR = Path('src')\n",
    "MODELS_DIR = Path('models')\n",
    "SRC_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from data_utils import create_dataset_processed, split_dataset\n",
    "from next_token_dataset import create_loaders\n",
    "from lstm_model import SimpleLSTMNextToken, count_parameters\n",
    "from eval_lstm import eval_rouge, autocomplete_examples\n",
    "from lstm_train import train_one_epoch, validate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8d4bf",
   "metadata": {},
   "source": [
    "#### Этап 1. Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fe8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Создан очищенный датасет, строк: 1600498.\n",
      "[OK] Созданы тренировочная, валидационная, тестовая выборки: 1280398, Val: 160050, Test: 160050.\n",
      "[OK] Созданы Dataset и DataLoader для обучения модели:\n",
      "Train batches: 4165 | Val batches: 510\n",
      "input_ids torch.Size([64, 32])\n",
      "labels torch.Size([64, 32])\n"
     ]
    }
   ],
   "source": [
    "# Очистка датасета и токенизация\n",
    "n_rows = create_dataset_processed()\n",
    "print(f\"[OK] Создан очищенный датасет, строк: {n_rows}.\")\n",
    "\n",
    "# Разбиение на выборки train/val/test\n",
    "n_train, n_val, n_test = split_dataset()\n",
    "print(f\"[OK] Созданы тренировочная, валидационная, тестовая выборки: {n_train}, Val: {n_val}, Test: {n_test}.\")\n",
    "\n",
    "# Создание Dataset и DataLoader\n",
    "(dsets, loaders, tokenizer) = create_loaders(\n",
    "    data_dir='data',\n",
    "    tokenizer_name='bert-base-multilingual-cased',\n",
    "    seq_len=32,\n",
    "    batch_size=64,\n",
    "    add_eos=True,\n",
    ")\n",
    "\n",
    "train_ds, val_ds, test_ds = dsets\n",
    "train_loader, val_loader, test_loader = loaders\n",
    "print(f\"[OK] Созданы Dataset и DataLoader для обучения модели:\")\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Просмотр одного батча\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    try:\n",
    "        shape = v.shape\n",
    "    except AttributeError:\n",
    "        shape = type(v)\n",
    "    print(k, shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1be797",
   "metadata": {},
   "source": [
    "#### Этап 2. Реализация рекуррентной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a80ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Модель создана. Кол-во обучаемых параметров: 46223227\n"
     ]
    }
   ],
   "source": [
    "# Создание модели, оптимизатора и функции потерь\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "model = SimpleLSTMNextToken(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    pad_idx=pad_id,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"[OK] Модель создана. Кол-во обучаемых параметров:\", count_parameters(model))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # labels у нас уже -100 на паддингах\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98942823",
   "metadata": {},
   "source": [
    "#### Этап 3. Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6cb87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Начинаем тренировку на 2 эпох:\n",
      "\n",
      "=== Epoch 1/2 ===\n",
      "  step   100 | loss 7.0981\n",
      "  step   200 | loss 6.7759\n",
      "  step   300 | loss 6.3413\n",
      "  step   400 | loss 6.0904\n",
      "  step   500 | loss 6.0971\n",
      "  step   600 | loss 5.6508\n",
      "  step   700 | loss 5.5767\n",
      "  step   800 | loss 5.2049\n",
      "  step   900 | loss 5.5405\n",
      "  step  1000 | loss 5.3453\n",
      "  step  1100 | loss 5.3905\n",
      "  step  1200 | loss 5.2709\n",
      "  step  1300 | loss 5.0088\n",
      "  step  1400 | loss 4.8302\n",
      "  step  1500 | loss 5.2144\n",
      "  step  1600 | loss 4.9991\n",
      "  step  1700 | loss 5.0433\n",
      "  step  1800 | loss 4.9426\n",
      "  step  1900 | loss 5.1331\n",
      "  step  2000 | loss 4.9505\n",
      "  step  2100 | loss 5.1774\n",
      "  step  2200 | loss 5.1229\n",
      "  step  2300 | loss 5.0882\n",
      "  step  2400 | loss 4.9805\n",
      "  step  2500 | loss 5.1598\n",
      "  step  2600 | loss 5.0963\n",
      "  step  2700 | loss 4.9539\n",
      "  step  2800 | loss 4.9368\n",
      "  step  2900 | loss 4.8557\n",
      "  step  3000 | loss 4.7712\n",
      "  step  3100 | loss 4.7384\n",
      "  step  3200 | loss 4.7804\n",
      "  step  3300 | loss 4.7190\n",
      "  step  3400 | loss 4.8225\n",
      "  step  3500 | loss 4.8961\n",
      "  step  3600 | loss 4.6808\n",
      "  step  3700 | loss 4.5984\n",
      "  step  3800 | loss 4.8765\n",
      "  step  3900 | loss 4.8411\n",
      "  step  4000 | loss 4.8116\n",
      "  step  4100 | loss 4.5805\n",
      "Epoch 1: train loss 5.2497 | val loss 4.9237 | time 2078.9s\n",
      "ROUGE: rouge1=0.0991 rouge2=0.0084 rougeL=0.0972 rougeLsum=0.0970\n",
      "Saved: models/lstm_next_token_e1_valloss4.924.pt\n",
      "→ Updated best: models/lstm_next_token_best.pt\n",
      "\n",
      "=== Epoch 2/2 ===\n",
      "  step   100 | loss 4.6983\n",
      "  step   200 | loss 4.8345\n",
      "  step   300 | loss 4.6243\n",
      "  step   400 | loss 4.9609\n",
      "  step   500 | loss 4.6240\n",
      "  step   600 | loss 4.6865\n",
      "  step   700 | loss 4.5229\n",
      "  step   800 | loss 4.7266\n",
      "  step   900 | loss 4.6911\n",
      "  step  1000 | loss 4.6719\n",
      "  step  1100 | loss 4.6337\n",
      "  step  1200 | loss 4.6784\n",
      "  step  1300 | loss 4.7500\n",
      "  step  1400 | loss 4.5158\n",
      "  step  1500 | loss 4.4990\n",
      "  step  1600 | loss 4.4982\n",
      "  step  1700 | loss 4.5509\n",
      "  step  1800 | loss 4.6271\n",
      "  step  1900 | loss 4.4757\n",
      "  step  2000 | loss 4.7107\n",
      "  step  2100 | loss 4.5958\n",
      "  step  2200 | loss 4.5323\n",
      "  step  2300 | loss 4.6988\n",
      "  step  2400 | loss 4.4164\n",
      "  step  2500 | loss 4.3897\n",
      "  step  2600 | loss 4.7262\n",
      "  step  2700 | loss 4.6526\n",
      "  step  2800 | loss 4.4552\n",
      "  step  2900 | loss 4.5471\n",
      "  step  3000 | loss 4.5670\n",
      "  step  3100 | loss 4.6103\n",
      "  step  3200 | loss 4.6215\n",
      "  step  3300 | loss 4.5247\n",
      "  step  3400 | loss 4.5196\n",
      "  step  3500 | loss 4.6452\n",
      "  step  3600 | loss 4.5528\n",
      "  step  3700 | loss 4.5461\n",
      "  step  3800 | loss 4.5566\n",
      "  step  3900 | loss 4.6094\n",
      "  step  4000 | loss 4.1794\n",
      "  step  4100 | loss 4.2907\n",
      "Epoch 2: train loss 4.5961 | val loss 4.8523 | time 1755.8s\n",
      "ROUGE: rouge1=0.0834 rouge2=0.0102 rougeL=0.0813 rougeLsum=0.0812\n",
      "Saved: models/lstm_next_token_e2_valloss4.852.pt\n",
      "→ Updated best: models/lstm_next_token_best.pt\n",
      "[OK] Модель обучена.\n"
     ]
    }
   ],
   "source": [
    "# Тренировка (train/val loss + ROUGE на валидации)\n",
    "from time import time\n",
    "\n",
    "epochs = 2\n",
    "log_every = 100\n",
    "rouge_max_batches = 10   # увеличить для более точной оценки (медленнее)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_ckpt_path = MODELS_DIR / \"lstm_next_token_best.pt\"\n",
    "\n",
    "print(f\"[OK] Начинаем тренировку на {epochs} эпох:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    t0 = time()\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, log_every=log_every)\n",
    "    val_loss   = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # ROUGE (3/4 → 1/4) на валидации\n",
    "    rouge = eval_rouge(model, val_loader, tokenizer, pad_id=pad_id, max_batches=rouge_max_batches)\n",
    "\n",
    "    dt = time() - t0\n",
    "    print(f\"Epoch {epoch}: train loss {train_loss:.4f} | val loss {val_loss:.4f} | time {dt:.1f}s\")\n",
    "    print(\"ROUGE:\", \" \".join([f\"{k}={v:.4f}\" for k, v in rouge.items()]))\n",
    "\n",
    "    # Сохраняем чекпойнт за эпоху\n",
    "    ckpt = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"hparams\": {\n",
    "            \"emb_dim\": 256,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 1,\n",
    "            \"dropout\": 0.1,\n",
    "            \"seq_len\": 32,\n",
    "            \"tokenizer_name\": \"bert-base-multilingual-cased\",\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            **{f\"rouge/{k}\": float(v) for k, v in rouge.items()},\n",
    "        },\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    ep_path = MODELS_DIR / f\"lstm_next_token_e{epoch}_valloss{val_loss:.3f}.pt\"\n",
    "    torch.save(ckpt, ep_path)\n",
    "    print(\"Saved:\", ep_path)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(ckpt, best_ckpt_path)\n",
    "        print(\"→ Updated best:\", best_ckpt_path)\n",
    "\n",
    "print(f\"[OK] Модель обучена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b81db",
   "metadata": {},
   "source": [
    "#### Этап 3. Тестовая метрика и примеры автодополнений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071506bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] ROUGE метрика:\n",
      "  rouge1: 0.1184\n",
      "  rouge2: 0.0402\n",
      "  rougeL: 0.1151\n",
      "  rougeLsum: 0.1153\n",
      "[OK] Примере автозаполнений:\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    errr morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh\n",
      "PREFIX:  errr morning s nice day sun is shining i have the entire day and night off woo really bad too\n",
      "TARGET:  ##th ache from brace oh\n",
      "PRED:    much i m so tired i m\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    ##rr morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well\n",
      "PREFIX:  ##rr morning s nice day sun is shining i have the entire day and night off woo really bad tooth\n",
      "TARGET:  ache from brace oh well\n",
      "PRED:    ##ng myself i m so tired\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well twee\n",
      "PREFIX:  morning s nice day sun is shining i have the entire day and night off woo really bad tooth ach\n",
      "TARGET:  ##e from brace oh well twee\n",
      "PRED:    ##e and i m so tire\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well tweet\n",
      "PREFIX:  s nice day sun is shining i have the entire day and night off woo really bad tooth ache\n",
      "TARGET:  from brace oh well tweet\n",
      "PRED:    i m so tired i m so\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well tweet me\n",
      "PREFIX:  nice day sun is shining i have the entire day and night off woo really bad tooth ache from\n",
      "TARGET:  brace oh well tweet me\n",
      "PRED:    the beach and i m so tired\n"
     ]
    }
   ],
   "source": [
    "# Загружаем лучший чекпойнт (если нужно)\n",
    "state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# ROUGE на тесте\n",
    "r_test = eval_rouge(model, test_loader, tokenizer, pad_id=pad_id, max_batches=rouge_max_batches)\n",
    "print(\"[OK] ROUGE метрика:\")\n",
    "for k, v in r_test.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# Несколько примеров (FULL / PREFIX / TARGET / PRED)\n",
    "print(\"[OK] Примере автозаполнений:\")\n",
    "autocomplete_examples(model, test_loader, tokenizer, pad_id=pad_id, num_examples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581e04d",
   "metadata": {},
   "source": [
    "#### Этап 4. Использование предобученного трансформера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] distilgpt2 на 400 примерах (CPU).\n",
      "[OK] Метрики ROUGE (distilgpt2):\n",
      "  rouge1: 0.0874\n",
      "  rouge2: 0.0150\n",
      "  rougeL: 0.0870\n",
      "  rougeLsum: 0.0870\n",
      "[OK] Пример автозаполнений:\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:   nice hair cut dude why were your students leaving in the middle of class 1st period\n",
      "PREFIX: nice hair cut dude why were your students leaving in the middle\n",
      "TARGET: of class 1st period\n",
      "PRED  : of a school day\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:   i have to know what you are coming up with for the ai tour\n",
      "PREFIX: i have to know what you are coming up with\n",
      "TARGET: for the ai tour\n",
      "PRED  : .\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:   i wanna see the new dress even though i didn t see the first dress\n",
      "PREFIX: i wanna see the new dress even though i didn t\n",
      "TARGET: see the first dress\n",
      "PRED  : ...\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:   hates payer contracts wasted energy or not it s true looks like it s going to be a long work day\n",
      "PREFIX: hates payer contracts wasted energy or not it s true looks like it s going\n",
      "TARGET: to be a long work day\n",
      "PRED  : to be in trouble.\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:   yes hubs knows he has a hottie by his side\n",
      "PREFIX: yes hubs knows he has a hottie\n",
      "TARGET: by his side\n",
      "PRED  : or a ch\n",
      "[OK] Оценка distilgpt2 завершена.\n"
     ]
    }
   ],
   "source": [
    "# Оценка качества предобученного distilgpt2 (трансформер)\n",
    "from eval_transformer_pipeline import eval_transformer_rouge\n",
    "\n",
    "rouge_scores = eval_transformer_rouge(\n",
    "    data_path=\"data/val.csv\",\n",
    "    text_col=\"text\",\n",
    "    max_samples=400\n",
    ")\n",
    "\n",
    "print(\"[OK] Оценка distilgpt2 завершена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeba70",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### **Этап 5. Формулирование выводов**\n",
    "\n",
    "##### **Метрики ROUGE**\n",
    "| Модель | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-Lsum |\n",
    "|:--|:--:|:--:|:--:|:--:|\n",
    "| **LSTM (моя модель)** | **0.1184** | **0.0402** | **0.1151** | **0.1153** |\n",
    "| **DistilGPT-2 (на 400 примерах)** | 0.0874 | 0.0150 | 0.0870 | 0.0870 |\n",
    "\n",
    "\n",
    "##### **Примеры автозаполнений**\n",
    "\n",
    "**LSTM (моя модель):**\n",
    "> **PREFIX:** “nice day sun is shining i have the entire day and night off woo really bad tooth ache from”  \n",
    "> **PRED:** “the beach and i m so tired”  \n",
    "> **TARGET:** “brace oh well tweet me”  \n",
    "Грамматически связна и стабильно дописывает последовательности, но часто повторяет шаблонные фразы (“i m so tired”, “i m so”).\n",
    "\n",
    "**DistilGPT-2:**\n",
    "> **PREFIX:** “nice hair cut dude why were your students leaving in the middle”  \n",
    "> **PRED:** “of a school day”  \n",
    "> **TARGET:** “of class 1st period”  \n",
    "Коротко и ближе к смыслу, но иногда выдаёт неполные или случайные окончания (*“.”*, *“...”*, *“or a ch”*).\n",
    "\n",
    "\n",
    "##### **Сравнение моделей**\n",
    "\n",
    "| Критерий | **LSTM** | **DistilGPT-2** |\n",
    "|:--|:--|:--|\n",
    "| **Точность (ROUGE)** | Выше — ближе к эталонным продолжениям | Ниже, но улучшается при увеличении кол-ва примеров |\n",
    "| **Смысловая связность** | Средняя, часто повторяет шаблонные фразы | Лучше улавливает контекст, но нестабильна |\n",
    "| **Грамматика и структура** | Стабильная и правильная | Иногда обрывает фразы |\n",
    "| **Ресурсы (CPU, память)** | Очень лёгкая и быстрая | Требует больше вычислений |\n",
    "\n",
    "\n",
    "##### **Рекомендации по применению**\n",
    "\n",
    "| Сценарий | Рекомендуемая модель | Причина |\n",
    "|-----------|---------------------|----------|\n",
    "| **Автодополнение в мобильных приложениях** | **LSTM** | Компактна, быстра, стабильно предсказывает текст, подходит для работы на CPU |\n",
    "| **Креативные и контекстные генерации (чат-боты, письма)** | **DistilGPT-2** | Более гибкая, естественная речь |\n",
    "\n",
    "\n",
    "##### **Итог**\n",
    "\n",
    "> **LSTM-модель** лучше подходит для **мобильных и офлайн-приложений**, где важна стабильность, компактность и скорость.  \n",
    "> **DistilGPT-2** эффективнее для **креативных и контекстных задач**, особенно при дальнейшем обучениина.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
