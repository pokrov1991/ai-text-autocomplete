{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[OK] Создан очищенный датасет /Users/alexeypokrovsky/Documents/Work/Learning/Development/Backend/Python/ai-text-autocomplete/data/dataset_processed.csv, строк: 1600498\n",
      "Processed rows: 1600498\n",
      "[OK] Созданы тренировочная, валидационная, тестовая выборки: 1280398, Val: 160050, Test: 160050\n",
      "Splits: 1280398 160050 160050\n",
      "[OK] Созданы Dataset и DataLoader для обучения модели.\n",
      "Train batches: 4165 | Val batches: 510\n",
      "batch:\n",
      "input_ids torch.Size([64, 32])\n",
      "labels torch.Size([64, 32])\n",
      "batch first tokens:\n",
      "tensor([10216, 10237, 35432, 10107, 10911, 15311, 10833, 22591, 10114, 11783])\n",
      "tensor([10237, 35432, 10107, 10911, 15311, 10833, 22591, 10114, 11783, 10950])\n",
      "Trainable params: 46223227\n",
      "\n",
      "=== Epoch 1/2 ===\n",
      "  step   100 | loss 6.9302\n",
      "  step   200 | loss 6.8629\n",
      "  step   300 | loss 6.4701\n",
      "  step   400 | loss 6.2009\n",
      "  step   500 | loss 5.9676\n",
      "  step   600 | loss 5.8187\n",
      "  step   700 | loss 5.7798\n",
      "  step   800 | loss 5.6252\n",
      "  step   900 | loss 5.4313\n",
      "  step  1000 | loss 5.3658\n",
      "  step  1100 | loss 5.4018\n",
      "  step  1200 | loss 5.2917\n",
      "  step  1300 | loss 4.9292\n",
      "  step  1400 | loss 5.2012\n",
      "  step  1500 | loss 5.2984\n",
      "  step  1600 | loss 5.2210\n",
      "  step  1700 | loss 5.0751\n",
      "  step  1800 | loss 5.3197\n",
      "  step  1900 | loss 5.0404\n",
      "  step  2000 | loss 4.9835\n",
      "  step  2100 | loss 5.0679\n",
      "  step  2200 | loss 5.0666\n",
      "  step  2300 | loss 4.8428\n",
      "  step  2400 | loss 4.9513\n",
      "  step  2500 | loss 5.0080\n",
      "  step  2600 | loss 5.0247\n",
      "  step  2700 | loss 5.0897\n",
      "  step  2800 | loss 5.0913\n",
      "  step  2900 | loss 4.7898\n",
      "  step  3000 | loss 4.9087\n",
      "  step  3100 | loss 4.7898\n",
      "  step  3200 | loss 4.7971\n",
      "  step  3300 | loss 4.7423\n",
      "  step  3400 | loss 4.5483\n",
      "  step  3500 | loss 4.8814\n",
      "  step  3600 | loss 4.8367\n",
      "  step  3700 | loss 4.6602\n",
      "  step  3800 | loss 4.7069\n",
      "  step  3900 | loss 4.8126\n",
      "  step  4000 | loss 4.8101\n",
      "  step  4100 | loss 4.7634\n",
      "Epoch 1: train loss 5.2731 | val loss 4.9341 | time 3579.3s\n",
      "ROUGE: rouge1=0.1022 rouge2=0.0105 rougeL=0.1010 rougeLsum=0.1009\n",
      "Saved: models/lstm_next_token_e1_valloss4.934.pt\n",
      "→ Updated best: models/lstm_next_token_best.pt\n",
      "\n",
      "=== Epoch 2/2 ===\n",
      "  step   100 | loss 4.6833\n",
      "  step   200 | loss 4.7030\n",
      "  step   300 | loss 4.9412\n",
      "  step   400 | loss 4.7235\n",
      "  step   500 | loss 4.7020\n",
      "  step   600 | loss 4.8892\n",
      "  step   700 | loss 4.7429\n",
      "  step   800 | loss 4.7720\n",
      "  step   900 | loss 4.7526\n",
      "  step  1000 | loss 4.6444\n",
      "  step  1100 | loss 4.6561\n",
      "  step  1200 | loss 4.6379\n",
      "  step  1300 | loss 4.5468\n",
      "  step  1400 | loss 4.7718\n",
      "  step  1500 | loss 4.3974\n",
      "  step  1600 | loss 4.6255\n",
      "  step  1700 | loss 4.7054\n",
      "  step  1800 | loss 4.5955\n",
      "  step  1900 | loss 4.6116\n",
      "  step  2000 | loss 4.5714\n",
      "  step  2100 | loss 4.5831\n",
      "  step  2200 | loss 4.6365\n",
      "  step  2300 | loss 4.7120\n",
      "  step  2400 | loss 4.4525\n",
      "  step  2500 | loss 4.5007\n",
      "  step  2600 | loss 4.7058\n",
      "  step  2700 | loss 4.6356\n",
      "  step  2800 | loss 4.6919\n",
      "  step  2900 | loss 4.5482\n",
      "  step  3000 | loss 4.6328\n",
      "  step  3100 | loss 4.5951\n",
      "  step  3200 | loss 4.7128\n",
      "  step  3300 | loss 4.4202\n",
      "  step  3400 | loss 4.4884\n",
      "  step  3500 | loss 4.5266\n",
      "  step  3600 | loss 4.5972\n",
      "  step  3700 | loss 4.6599\n",
      "  step  3800 | loss 4.6772\n",
      "  step  3900 | loss 4.4774\n",
      "  step  4000 | loss 4.5703\n",
      "  step  4100 | loss 4.3475\n",
      "Epoch 2: train loss 4.6100 | val loss 4.8545 | time 2083.2s\n",
      "ROUGE: rouge1=0.1009 rouge2=0.0095 rougeL=0.0977 rougeLsum=0.0975\n",
      "Saved: models/lstm_next_token_e2_valloss4.854.pt\n",
      "→ Updated best: models/lstm_next_token_best.pt\n",
      "\n",
      "ROUGE on test:\n",
      "  rouge1: 0.1342\n",
      "  rouge2: 0.0419\n",
      "  rougeL: 0.1322\n",
      "  rougeLsum: 0.1321\n",
      "\n",
      "Examples:\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    errr morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh\n",
      "PREFIX:  errr morning s nice day sun is shining i have the entire day and night off woo really bad too\n",
      "TARGET:  ##th ache from brace oh\n",
      "PRED:    much i m not going to be a\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    ##rr morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well\n",
      "PREFIX:  ##rr morning s nice day sun is shining i have the entire day and night off woo really bad tooth\n",
      "TARGET:  ache from brace oh well\n",
      "PRED:    ##brush i m so tired and\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    morning s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well twee\n",
      "PREFIX:  morning s nice day sun is shining i have the entire day and night off woo really bad tooth ach\n",
      "TARGET:  ##e from brace oh well twee\n",
      "PRED:    ##e i m so tired and i\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    s nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well tweet\n",
      "PREFIX:  s nice day sun is shining i have the entire day and night off woo really bad tooth ache\n",
      "TARGET:  from brace oh well tweet\n",
      "PRED:    i m so tired and i m\n",
      "————————————————————————————————————————————————————————————\n",
      "FULL:    nice day sun is shining i have the entire day and night off woo really bad tooth ache from brace oh well tweet me\n",
      "PREFIX:  nice day sun is shining i have the entire day and night off woo really bad tooth ache from\n",
      "TARGET:  brace oh well tweet me\n",
      "PRED:    the gymnastics and i m\n"
     ]
    }
   ],
   "source": [
    "# Автоперезагрузка импортов\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Отключаем параллелизм в токенизаторах (иначе могут быть проблемы с многопроцессорностью)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Подключаем нужные пути\n",
    "SRC_DIR = Path('src')\n",
    "MODELS_DIR = Path('models')\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from data_utils import create_dataset_processed, split_dataset\n",
    "from next_token_dataset import create_loaders\n",
    "from lstm_model import SimpleLSTMNextToken, count_parameters\n",
    "from eval_lstm import eval_rouge, autocomplete_examples\n",
    "from lstm_train import train_one_epoch, validate\n",
    "\n",
    "# Очистка датасета и токенизация\n",
    "n_rows = create_dataset_processed()\n",
    "print(f\"[OK] Создан очищенный датасет, строк: {n_rows}.\")\n",
    "\n",
    "# Разбиение на выборки train/val/test\n",
    "n_train, n_val, n_test = split_dataset()\n",
    "print(f\"[OK] Созданы тренировочная, валидационная, тестовая выборки: {n_train}, Val: {n_val}, Test: {n_test}.\")\n",
    "\n",
    "# Создание Dataset и DataLoader\n",
    "(dsets, loaders, tokenizer) = create_loaders(\n",
    "    data_dir='data',\n",
    "    tokenizer_name='bert-base-multilingual-cased',\n",
    "    seq_len=32,\n",
    "    batch_size=64,\n",
    "    add_eos=True,\n",
    ")\n",
    "\n",
    "train_ds, val_ds, test_ds = dsets\n",
    "train_loader, val_loader, test_loader = loaders\n",
    "print(f\"[OK] Созданы Dataset и DataLoader для обучения модели:\")\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# Создание модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "model = SimpleLSTMNextToken(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    pad_idx=pad_id,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"[OK] Модель создана. Кол-во обучаемых параметры:\", count_parameters(model))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # labels у нас уже -100 на паддингах\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)\n",
    "\n",
    "# === Тренировка: выводим train loss, val loss и ROUGE на вал. ===\n",
    "from time import time\n",
    "\n",
    "epochs = 2\n",
    "log_every = 100\n",
    "rouge_max_batches = 10   # увеличить для более точной оценки (медленнее)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_ckpt_path = MODELS_DIR / \"lstm_next_token_best.pt\"\n",
    "\n",
    "print(f\"[OK] Начинаем тренировку на {epochs} эпох:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
    "    t0 = time()\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, log_every=log_every)\n",
    "    val_loss   = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # ROUGE (3/4 → 1/4) на валидации\n",
    "    rouge = eval_rouge(model, val_loader, tokenizer, pad_id=pad_id, max_batches=rouge_max_batches)\n",
    "\n",
    "    dt = time() - t0\n",
    "    print(f\"Epoch {epoch}: train loss {train_loss:.4f} | val loss {val_loss:.4f} | time {dt:.1f}s\")\n",
    "    print(\"ROUGE:\", \" \".join([f\"{k}={v:.4f}\" for k, v in rouge.items()]))\n",
    "\n",
    "    # Сохраняем чекпойнт за эпоху\n",
    "    ckpt = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"hparams\": {\n",
    "            \"emb_dim\": 256,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 1,\n",
    "            \"dropout\": 0.1,\n",
    "            \"seq_len\": 32,\n",
    "            \"tokenizer_name\": \"bert-base-multilingual-cased\",\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            **{f\"rouge/{k}\": float(v) for k, v in rouge.items()},\n",
    "        },\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    ep_path = MODELS_DIR / f\"lstm_next_token_e{epoch}_valloss{val_loss:.3f}.pt\"\n",
    "    torch.save(ckpt, ep_path)\n",
    "    print(\"Saved:\", ep_path)\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(ckpt, best_ckpt_path)\n",
    "        print(\"→ Updated best:\", best_ckpt_path)\n",
    "\n",
    "print(f\"[OK] Модель обучена.\")\n",
    "\n",
    "# === Тестовая метрика ROUGE + примеры автодополнений ===\n",
    "# Загружаем лучший чекпойнт (если нужно)\n",
    "state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "model.to(device).eval()\n",
    "\n",
    "# ROUGE на тесте\n",
    "r_test = eval_rouge(model, test_loader, tokenizer, pad_id=pad_id, max_batches=rouge_max_batches)\n",
    "print(\"[OK] ROUGE метрика:\")\n",
    "for k, v in r_test.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# Несколько примеров (FULL / PREFIX / TARGET / PRED)\n",
    "print(\"[OK] Примере автозаполнений:\")\n",
    "autocomplete_examples(model, test_loader, tokenizer, pad_id=pad_id, num_examples=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
